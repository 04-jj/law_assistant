import torch
from transformers import AutoModelForCausalLM, AutoTokenizer


def simple_chat_demo():
    # æ¨¡å‹è·¯å¾„
    model_path = r"C:\Users\PC\.cache\modelscope\hub\models\deepseek-ai\DeepSeek-R1-Distill-Qwen-1.5B"

    print("æ­£åœ¨åŠ è½½æ¨¡å‹å’Œtokenizer...")

    try:
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼")
        print(f"æ¨¡å‹è®¾å¤‡: {model.device}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    print("\nğŸ¤– å¼€å§‹å¯¹è¯ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ æ‚¨: ").strip()

        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("å†è§ï¼ğŸ‘‹")
            break

        if not user_input:
            continue

        try:
            # ç®€å•æ„å»ºprompt
            prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"

            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            # ç”Ÿæˆå›å¤
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

            # è§£ç å›å¤
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # åªæå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
            response = response.split("åŠ©æ‰‹:")[-1].strip()

            print(f"ğŸ¤– åŠ©æ‰‹: {response}")

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")


if __name__ == "__main__":
    simple_chat_demo()